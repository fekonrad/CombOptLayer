{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fekonrad/CombOptLayer/blob/main/Demo/COptLayer_Warcraft_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VncOI7_MobSJ"
      },
      "source": [
        "# Using the COptLayer\n",
        "First, let's install the COptLayer repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UzGr0dUojQd",
        "outputId": "44324499-22e2-426d-8828-e6578c3f5fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CombOptLayer'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 96 (delta 31), reused 25 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (96/96), 40.63 KiB | 5.80 MiB/s, done.\n",
            "Resolving deltas: 100% (31/31), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/fekonrad/CombOptLayer.git\n",
        "!cd CombOptLayer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPf2sH9UzA6X"
      },
      "source": [
        "... and install all necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCiXqcxHzDKc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import CombOptLayer\n",
        "from CombOptLayer import COptLayer\n",
        "from CombOptLayer.losses import PerturbedLoss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFAgfvjAoIYD"
      },
      "source": [
        "# The Task\n",
        "...\n",
        "\n",
        "This is a short demo on how to use the COptLayer for the toy problem of finding shortest paths on Warcraft maps. To illustrate what the maps, graphs and paths look like, see the plots below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a3OywNyqPOI"
      },
      "outputs": [],
      "source": [
        "class WarcraftPaths(Dataset):\n",
        "    def __init__(self, map_path, cost_path, paths_path):\n",
        "        super().__init__()\n",
        "        self.maps = torch.tensor(np.load(map_path), dtype=torch.float32).permute(0, 3, 1, 2)\n",
        "        self.costs = torch.tensor(np.load(cost_path), dtype=torch.float32)\n",
        "        self.shortest_paths = torch.tensor(np.load(paths_path), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.maps.shape[0]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.maps[item], self.costs[item], self.shortest_paths[item]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TTGAnc1oEvj"
      },
      "outputs": [],
      "source": [
        "# TODO: Load and plot a sample map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-PRcUyuo3oN"
      },
      "source": [
        "## The Model (CNN)\n",
        "We implement a very basic CNN to estimate the vertex costs of the map. Since this problem is not very complex, a relatively small model would suffice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvN6s7omo4kN"
      },
      "outputs": [],
      "source": [
        "class VertexWeightCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # define architecture ...\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding='same')\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, padding='same')\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, padding='same')\n",
        "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, padding='same')\n",
        "        self.conv6 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=2, padding='same')\n",
        "        self.final_layer = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=2, padding='same')\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param img: torch.tensor of shape (b, c, h, w)      (in our case c=3, h=w=96)\n",
        "        :return: torch.tensor of shape (b, h', w')      (in our case h'=w'=12)\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)             # (b, 16, 96, 96)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.conv2(x)               # (b, 32, 96, 96)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)        # (b, 32, 48, 48)\n",
        "\n",
        "        x = self.conv3(x)             # (b, 32, 48, 48)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.conv4(x)               # (b, 32, 96, 96)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)        # (b, 32, 24, 24)\n",
        "\n",
        "        x = self.conv5(x)             # (b, 32, 24, 24)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.conv6(x)               # (b, 32, 24, 24)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = nn.MaxPool2d(kernel_size=2)(x)        # (b, 32, 12, 12)\n",
        "\n",
        "        return nn.Softplus()(self.final_layer(x)).squeeze(1)  # Softplus to make weights non-negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQiIpKtjolR5"
      },
      "source": [
        "## The Combinatorial Solver (Dijkstra)\n",
        "Now we just have to implement our solver for finding shortest paths (given vertex weights) on 2D grids, where  the possible moves at each point are up, down, left, right and diagonal.\n",
        "\n",
        "We have to make sure that the input of our solver is compatible with what our statistical model (the CNN) returns as outputs, i.e. in this case a `torch.tensor` of shape `(b, 1, h, w)`.\n",
        "\n",
        "**Remark:**\n",
        "The COptLayer will have to solve multiple instances of the combinatorial problem at once. Therefore it is clever to try to implement the solver using parallelizable operations, instead of trying to solve the problems sequentially. This means either sticking to what `torch` already has implemented or writing your own parallelized methods e.g. using CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToS0IRAyoy4k"
      },
      "outputs": [],
      "source": [
        "def parallel_dijkstra(vertex_weights: torch.Tensor, max_iterations: int = None):\n",
        "    \"\"\"\n",
        "    Parallel Dijkstra-like solver for multiple instances on a 2D grid.\n",
        "\n",
        "    :param vertex_weights: Tensor of shape (b, h, w) representing vertex weights.\n",
        "    :param max_iterations: Maximum number of iterations to run. If None, set to h + w.\n",
        "    :return: Tensor of shape (b, h, w) indicating the path taken (1s on the path, 0s elsewhere).\n",
        "    \"\"\"\n",
        "    b, h, w = vertex_weights.shape\n",
        "    device = vertex_weights.device\n",
        "\n",
        "    # Initialize distance tensor with infinity and set the start position\n",
        "    distances = torch.full((b, h, w), float('inf'), device=device)\n",
        "    distances[:, 0, 0] = vertex_weights[:, 0, 0]\n",
        "\n",
        "    # Initialize predecessor tensors to keep track of paths\n",
        "    predecessors_x = torch.full((b, h, w), -1, dtype=torch.long, device=device)\n",
        "    predecessors_y = torch.full((b, h, w), -1, dtype=torch.long, device=device)\n",
        "\n",
        "    # Define shifts for 8-connected neighborhood\n",
        "    shifts = [(-1, 0), (1, 0), (0, -1), (0, 1),\n",
        "              (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "\n",
        "    # Determine the number of iterations\n",
        "    if max_iterations is None:\n",
        "        max_iterations = h + w  # Upper bound for grid-based paths\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        updated = False\n",
        "        current_distances = distances.clone()\n",
        "\n",
        "        for dx, dy in shifts:\n",
        "            # Shift the distances tensor\n",
        "            shifted_distances = F.pad(current_distances, (1, 1, 1, 1), mode='constant', value=float('inf'))\n",
        "            if dx < 0:\n",
        "                shifted_distances = shifted_distances[:, :h, 1:w+1]\n",
        "            elif dx > 0:\n",
        "                shifted_distances = shifted_distances[:, 2:h+2, 1:w+1]\n",
        "            else:\n",
        "                shifted_distances = shifted_distances[:, 1:h+1, 1:w+1]\n",
        "\n",
        "            if dy < 0:\n",
        "                shifted_distances = shifted_distances[:, :, :w]\n",
        "            elif dy > 0:\n",
        "                shifted_distances = shifted_distances[:, :, 2:w+2]\n",
        "            else:\n",
        "                shifted_distances = shifted_distances[:, :, 1:w+1]\n",
        "\n",
        "            # Compute the new possible distances\n",
        "            new_distances = shifted_distances + vertex_weights\n",
        "\n",
        "            # Update the distances tensor\n",
        "            mask = new_distances < distances\n",
        "            if mask.any():\n",
        "                distances = torch.where(mask, new_distances, distances)\n",
        "                # Update predecessors\n",
        "                px = torch.where(mask, torch.full_like(predecessors_x, torch.clamp(torch.arange(b, device=device)[:, None, None], max=b-1)), predecessors_x)\n",
        "                py = torch.where(mask, torch.full_like(predecessors_y, torch.clamp(torch.arange(w, device=device)[None, :, None], max=w-1)), predecessors_y)\n",
        "                updated = True\n",
        "\n",
        "        if not updated:\n",
        "            break\n",
        "\n",
        "    # Backtracking to find the paths\n",
        "    path_tensor = torch.zeros_like(vertex_weights, dtype=torch.float32)\n",
        "\n",
        "    # Start from the bottom-right corner\n",
        "    x_coords = torch.full((b,), h - 1, dtype=torch.long, device=device)\n",
        "    y_coords = torch.full((b,), w - 1, dtype=torch.long, device=device)\n",
        "\n",
        "    for _ in range(h + w):\n",
        "        # Set the path\n",
        "        path_tensor[torch.arange(b), x_coords, y_coords] = 1\n",
        "\n",
        "        # Get predecessor coordinates\n",
        "        prev_x = predecessors_x[torch.arange(b), x_coords, y_coords]\n",
        "        prev_y = predecessors_y[torch.arange(b), x_coords, y_coords]\n",
        "\n",
        "        # Check if we've reached the start\n",
        "        if (prev_x == -1) & (prev_y == -1):\n",
        "            break\n",
        "\n",
        "        # Update coordinates\n",
        "        x_coords = prev_x\n",
        "        y_coords = prev_y\n",
        "\n",
        "    return path_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvdwy9vSp8Rx"
      },
      "source": [
        "Now we have everything we need to build the model!\n",
        "Here we use the PerturbedLoss with our implemented solver to train the CNN.\n",
        "\n",
        "*Note:*\n",
        "Technically, we should require the vertex weights to be non-negative, in order to guarantee convergence of the solver. Thus, one could/should use the \"Multiplicative Perturbation discussed in the paper (reference here). Here we simply use the additive perturbation (which might lead to some weights becoming negative!) and the experiments still seem to work fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mncNwB1EqBzC"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = VertexWeightCNN().to(DEVICE)\n",
        "solver = dijkstra\n",
        "loss_fn = PerturbedLoss(solver, objective='min', num_samples=10, smoothing=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAF093WSqFNc"
      },
      "source": [
        "The training routine now works like any other training routine in torch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaYDNHfoqTU_"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement training routine and monitor sample map.\n",
        "epochs = 10\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# TODO: Maybe fix the paths here.\n",
        "data = WarcraftPaths(\"warcraft-maps-shortest-paths/test_maps.npy\",\n",
        "                     \"warcraft-maps-shortest-paths/test_vertex_weights.npy\",\n",
        "                     \"warcraft-maps-shortest-paths/test_shortest_paths.npy\")\n",
        "\n",
        "dataloader = DataLoader(data, batch_size=16, shuffle=True)\n",
        "steps = 0\n",
        "loss_vals = []\n",
        "\n",
        "for _ in range(epochs):\n",
        "    for maps, weights, paths in dataloader:\n",
        "        maps, weights, paths = maps.to(DEVICE), weights.to(DEVICE), paths.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        vertex_weight_pred = model(maps)\n",
        "        paths_pred = solver(vertex_weight_pred.squeeze(1))\n",
        "        loss = loss_fn(vertex_weight_pred, paths)\n",
        "        loss_val = loss.item()\n",
        "        loss_vals.append(loss_val)\n",
        "        print(f\"Loss after {steps} Steps: {loss_val}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "\n",
        "    fig, ax = plt.subplots(ncols=2, nrows=2)\n",
        "    ax[0, 0].imshow(weights[0].cpu().detach().numpy())\n",
        "    ax[0, 1].imshow(model(maps).cpu().detach().numpy()[0])\n",
        "    ax[1, 0].imshow(paths[0].cpu().detach().numpy())\n",
        "    ax[1, 1].imshow(paths_pred[0].cpu().detach().numpy())\n",
        "    plt.show()\n",
        "\n",
        "plt.plot(loss_vals)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Perturbed Loss\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qztXoLcRqZiA"
      },
      "source": [
        "# Results\n",
        "..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMB88y60HUU15JjK80gfSS5",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
